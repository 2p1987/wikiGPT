{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a Generative Pretrained Transformer (GPT) model?\n",
    "\n",
    "The goal of this notebook is to explore the main aspects around training a generative AI model for text.\\\n",
    "We will review the main concepts & steps for training and talk also about how the prediction of new content happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"]).strip().decode(\"utf-8\")\n",
    "os.chdir(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The different phases of model training \n",
    "\n",
    "### Pre-training\n",
    "### Supervised fine-tuning\n",
    "### Alignement with human preferences (RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wiki english\n",
    "import json\n",
    "with open('wikiGPT/data/shuffled_shards/shard_0.json', 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nowa Kiszewa  is a village in the administrative district of Gmina Kościerzyna, within Kościerzyna County, Pomeranian Voivodeship, in northern Poland. It lies approximately  south-east of Kościerzyna '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"content\"][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 21, 34, 2, 33, 4, 34, 32, 18, 25, 29, 4, 18, 4, 10, 15, 2, 4, 28, 30, 20, 21, 27, 2]\n",
      "<unk>et<unk>s train a GP<unk> model<unk>\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(data[\"content\"])))\n",
    "vocab_size = len(vocab)\n",
    "def create_naive_tokenizer(text):\n",
    "    tokenizer = {vocab[i]: i + 3 for i in range(len(vocab))}\n",
    "    tokenizer[\"<s>\"] = 0\n",
    "    tokenizer[\"</s>\"] = 1\n",
    "    tokenizer[\"<unk>\"] = 2\n",
    "    detokenizer = {v: k for k, v in tokenizer.items()}\n",
    "    return tokenizer, detokenizer\n",
    "\n",
    "naive_tokenizer, naive_detokenizer = create_naive_tokenizer(data[\"content\"])\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tokenizer.get(letter, 2) for letter in text]\n",
    "\n",
    "def detokenize(tokens, detokenizer):\n",
    "    return \"\".join([detokenizer.get(token, \"<unk>\") for token in tokens])\n",
    "\n",
    "tokens = tokenize(\"Let's train a GPT model!\", naive_tokenizer)\n",
    "print(tokens)\n",
    "print(detokenize(tokens, naive_detokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A modern tokenizer based on the Byte Pair Encoding (BPE) algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love to learn about AI!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Byte pair encoding => use the data to tell how to tokenize\n",
    "\n",
    "# From a corpus of text to train the tokenizer\n",
    "# Choose a vocabulary size: i.e. the maximum number of tokens our vocabulary can have\n",
    "\n",
    "# Start with a character-level tokenizer from the corpus\n",
    "\"I loave to learn about AI\" # => [' ', 'A', 'I', 'a', 'b', 'e', 'l', 'n', 'o', 'r', 't', 'u', 'v']\n",
    "\n",
    "# Count the frequency of pairs of characters in the corpus\n",
    "\"I love to learn about AI!\" # => {\"I \": 1, \" l\": 1, \"lo\": 1, \"ov\": 1, ...}\n",
    "\n",
    "# Merge the most frequent pair of characters into a single token\n",
    "\"I love to learn about AI!\" # => {\"I \": 1, \" l\": 1, \"lo\": 1, \"ov\": 1, ...}\n",
    "\n",
    "# Repeat until the vocabulary size is reached\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318, 3832, 298, 3178, 900, 14788, 19086]\n",
      "I love to learn about AI!\n"
     ]
    }
   ],
   "source": [
    "from wikiGPT.tokenize import Tokenizer\n",
    "tokenizer = Tokenizer(Path(\"wikiGPT/tokenizers/tok32000.model\"))\n",
    "\n",
    "tokens = tokenizer.encode(\"I love to learn about AI!\", bos=False, eos=False)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love to learn about AI!',\n",
       " \"Let's train a GPT model!\",\n",
       " 'Nowa Kiszewa  is a village in ']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A batch of data from human to machine reading\n",
    "\n",
    "# A batch is composed of two dimensions: the number of samples we are going to pass to the model and the context length\n",
    "\n",
    "corpus = [\n",
    "    \"I love to learn about AI!\",\n",
    "    \"Let's train a GPT model!\",\n",
    "    data[\"content\"][:30],\n",
    "]\n",
    "corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[318, 3832, 298, 3178, 900, 14788, 19086],\n",
       " [6974, 19055, 19006, 3820, 262, 11867, 19028, 3005, 19086],\n",
       " [344, 4397, 385, 273, 15544, 19000, 351, 262, 1429, 280]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "context_length = 4\n",
    "\n",
    "# let's tokenize our text\n",
    "tokenized_corpus = [\n",
    "    tokenizer.encode(text, False, False) for text in corpus\n",
    "]\n",
    "\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected sample: 1\n"
     ]
    }
   ],
   "source": [
    "# Creating a first batch for the next token prediction\n",
    "import random\n",
    "sample = random.choice(range(len(tokenized_corpus)))\n",
    "print(f\"Selected sample: {sample}\")\n",
    "\n",
    "batch = tokenized_corpus[sample][:context_length + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6974, 19055, 19006, 3820, 262]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token(s) [6974] need to predict 19055\n",
      "token(s) [6974, 19055] need to predict 19006\n",
      "token(s) [6974, 19055, 19006] need to predict 3820\n",
      "token(s) [6974, 19055, 19006, 3820] need to predict 262\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_length):\n",
    "    print(f\"token(s) {batch[:i+1]} need to predict {batch[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword(s) 'Let' need to predict '''\n",
      "subword(s) 'Let'' need to predict 's'\n",
      "subword(s) 'Let's' need to predict 'train'\n",
      "subword(s) 'Let's train' need to predict 'a'\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_length):    \n",
    "    print(f\"\"\"subword(s) '{tokenizer.decode(batch[:i+1])}' need to predict '{tokenizer.decode(batch[i+1])}'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the model\n",
    "\n",
    "### Transformer architecture:\n",
    "#### We are going to focus on the embedding layer, talk about the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1192, 0.3502, 0.7753, 0.6743, 0.4592, 0.7192],\n",
      "        [0.7392, 0.7237, 0.7500, 0.4516, 0.7722, 0.8238],\n",
      "        [0.8384, 0.2815, 0.0946, 0.2193, 0.4113, 0.9652]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "### The embedding layer\n",
    "\n",
    "#### This is the initial layer that will transform the token (int) into a vector\n",
    "vocab = [0, 1, 2]\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 6\n",
    "\n",
    "embedding_layer = torch.rand(vocab_size, embedding_size)\n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 6])\n",
      "tensor([[[0.7392, 0.7237, 0.7500, 0.4516, 0.7722, 0.8238],\n",
      "         [0.1192, 0.3502, 0.7753, 0.6743, 0.4592, 0.7192],\n",
      "         [0.1192, 0.3502, 0.7753, 0.6743, 0.4592, 0.7192],\n",
      "         [0.8384, 0.2815, 0.0946, 0.2193, 0.4113, 0.9652]]])\n"
     ]
    }
   ],
   "source": [
    "batch = [1, 0, 0, 2]\n",
    "embedding = torch.zeros(batch_size, context_length, embedding_size)\n",
    "for i, token in enumerate(batch):\n",
    "    embedding[0, i] = embedding_layer[token]\n",
    "\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7392, 0.7237, 0.7500, 0.4516, 0.7722, 0.8238],\n",
       "        [0.1192, 0.3502, 0.7753, 0.6743, 0.4592, 0.7192],\n",
       "        [0.1192, 0.3502, 0.7753, 0.6743, 0.4592, 0.7192],\n",
       "        [0.8384, 0.2815, 0.0946, 0.2193, 0.4113, 0.9652]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_one_hot_encoded = torch.Tensor(\n",
    "    [\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_one_hot_encoded @ embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### What is the attention mechanism?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we measure the performance of a model? The cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we actually train the model?\n",
    "#### What is gradient descent and backpropagation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A note on model sizes and required training computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How do we make a prediction with the model?\n",
    "#### probabilistic sampling\n",
    "#### inefficiency of the attention mechanism at inference time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
