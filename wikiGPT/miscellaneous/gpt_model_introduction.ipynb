{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a Generative Pretrained Transformer (GPT) model?\n",
    "\n",
    "The goal of this notebook is to explore the main aspects around training a generative AI model for text.\\\n",
    "We will review the main concepts & steps for training and talk also about how the prediction of new content happens.\n",
    "\n",
    "Topics:\n",
    "- Tokenization\n",
    "- Main model components\n",
    "- How the model performance is evaluated\n",
    "- How the model is trained\n",
    "- How the model is used to create new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = (\n",
    "    subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n",
    "    .strip()\n",
    "    .decode(\"utf-8\")\n",
    ")\n",
    "os.chdir(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The different phases of model training\n",
    "\n",
    "### Pre-training\n",
    "### Supervised fine-tuning\n",
    "### Alignement with human preferences (RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wiki english\n",
    "import json\n",
    "\n",
    "with open(\"wikiGPT/data/shuffled_shards/shard_0.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nowa Kiszewa  is a village in the administrative district of Gmina Kościerzyna, within Kościerzyna County, Pomeranian Voivodeship, in northern Poland. It lies approximately  south-east of Kościerzyna '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"content\"][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 21, 34, 2, 33, 4, 34, 32, 18, 25, 29, 4, 18, 4, 10, 15, 2, 4, 28, 30, 20, 21, 27, 2]\n",
      "<unk>et<unk>s train a GP<unk> model<unk>\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(data[\"content\"])))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "def create_naive_tokenizer(text):\n",
    "    tokenizer = {vocab[i]: i + 3 for i in range(len(vocab))}\n",
    "    tokenizer[\"<s>\"] = 0\n",
    "    tokenizer[\"</s>\"] = 1\n",
    "    tokenizer[\"<unk>\"] = 2\n",
    "    detokenizer = {v: k for k, v in tokenizer.items()}\n",
    "    return tokenizer, detokenizer\n",
    "\n",
    "\n",
    "naive_tokenizer, naive_detokenizer = create_naive_tokenizer(data[\"content\"])\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tokenizer.get(letter, 2) for letter in text]\n",
    "\n",
    "\n",
    "def detokenize(tokens, detokenizer):\n",
    "    return \"\".join([detokenizer.get(token, \"<unk>\") for token in tokens])\n",
    "\n",
    "\n",
    "tokens = tokenize(\"Let's train a GPT model!\", naive_tokenizer)\n",
    "print(tokens)\n",
    "print(detokenize(tokens, naive_detokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A modern tokenizer based on the Byte Pair Encoding (BPE) algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love to learn about AI!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Byte pair encoding => use the data to tell how to tokenize\n",
    "\n",
    "# From a corpus of text to train the tokenizer\n",
    "# Choose a vocabulary size: i.e. the maximum number of tokens our vocabulary can have\n",
    "\n",
    "# Start with a character-level tokenizer from the corpus\n",
    "\"I loave to learn about AI\"  # => [' ', 'A', 'I', 'a', 'b', 'e', 'l', 'n', 'o', 'r', 't', 'u', 'v']\n",
    "\n",
    "# Count the frequency of pairs of characters in the corpus\n",
    "\"I love to learn about AI!\"  # => {\"I \": 1, \" l\": 1, \"lo\": 1, \"ov\": 1, ...}\n",
    "\n",
    "# Merge the most frequent pair of characters into a single token\n",
    "\"I love to learn about AI!\"  # => {\"I \": 1, \" l\": 1, \"lo\": 1, \"ov\": 1, ...}\n",
    "\n",
    "# Repeat until the vocabulary size is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318, 3832, 298, 3178, 900, 14788, 19086]\n",
      "I love to learn about AI!\n"
     ]
    }
   ],
   "source": [
    "from wikiGPT.tokenize import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(Path(\"wikiGPT/tokenizers/tok32000.model\"))\n",
    "\n",
    "tokens = tokenizer.encode(\"I love to learn about AI!\", bos=False, eos=False)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love to learn about AI!',\n",
       " \"Let's train a GPT model!\",\n",
       " 'Nowa Kiszewa  is a village in ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A batch of data from human to machine reading\n",
    "\n",
    "# A batch is composed of two dimensions: the number of samples we are going to pass to the model and the context length\n",
    "\n",
    "corpus = [\n",
    "    \"I love to learn about AI!\",\n",
    "    \"Let's train a GPT model!\",\n",
    "    data[\"content\"][:30],\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[318, 3832, 298, 3178, 900, 14788, 19086],\n",
       " [6974, 19055, 19006, 3820, 262, 11867, 19028, 3005, 19086],\n",
       " [344, 4397, 385, 273, 15544, 19000, 351, 262, 1429, 280]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "context_length = 4\n",
    "\n",
    "# let's tokenize our text\n",
    "tokenized_corpus = [tokenizer.encode(text, False, False) for text in corpus]\n",
    "\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected sample: 0\n"
     ]
    }
   ],
   "source": [
    "# Creating a first batch for the next token prediction\n",
    "import random\n",
    "\n",
    "sample = random.choice(range(len(tokenized_corpus)))\n",
    "print(f\"Selected sample: {sample}\")\n",
    "\n",
    "batch = tokenized_corpus[sample][: context_length + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[318, 3832, 298, 3178, 900]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token(s) [318] need to predict 3832\n",
      "token(s) [318, 3832] need to predict 298\n",
      "token(s) [318, 3832, 298] need to predict 3178\n",
      "token(s) [318, 3832, 298, 3178] need to predict 900\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_length):\n",
    "    print(f\"token(s) {batch[:i+1]} need to predict {batch[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword(s) 'I' need to predict 'love'\n",
      "subword(s) 'I love' need to predict 'to'\n",
      "subword(s) 'I love to' need to predict 'learn'\n",
      "subword(s) 'I love to learn' need to predict 'about'\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_length):\n",
    "    print(\n",
    "        f\"\"\"subword(s) '{tokenizer.decode(batch[:i+1])}' need to predict '{tokenizer.decode(batch[i+1])}'\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the model\n",
    "\n",
    "### Transformer architecture:\n",
    "#### We are going to focus on the embedding layer, talk about the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6752, 0.7663, 0.9452, 0.7748, 0.2242, 0.4582],\n",
      "        [0.4008, 0.8364, 0.6558, 0.8886, 0.4826, 0.1786],\n",
      "        [0.5440, 0.5719, 0.5054, 0.6391, 0.1919, 0.5260]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "### The embedding layer\n",
    "\n",
    "#### This is the initial layer that will transform the token (int) into a vector\n",
    "vocab = [0, 1, 2]\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 6\n",
    "\n",
    "embedding_layer = torch.rand(vocab_size, embedding_size)\n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 6])\n",
      "tensor([[[0.4008, 0.8364, 0.6558, 0.8886, 0.4826, 0.1786],\n",
      "         [0.6752, 0.7663, 0.9452, 0.7748, 0.2242, 0.4582],\n",
      "         [0.6752, 0.7663, 0.9452, 0.7748, 0.2242, 0.4582],\n",
      "         [0.5440, 0.5719, 0.5054, 0.6391, 0.1919, 0.5260]]])\n"
     ]
    }
   ],
   "source": [
    "batch = [1, 0, 0, 2]\n",
    "batch_size = 1\n",
    "context_length = 4\n",
    "embedding = torch.zeros(batch_size, context_length, embedding_size)\n",
    "for i, token in enumerate(batch):\n",
    "    embedding[0, i] = embedding_layer[token]\n",
    "\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4008, 0.8364, 0.6558, 0.8886, 0.4826, 0.1786],\n",
       "        [0.6752, 0.7663, 0.9452, 0.7748, 0.2242, 0.4582],\n",
       "        [0.6752, 0.7663, 0.9452, 0.7748, 0.2242, 0.4582],\n",
       "        [0.5440, 0.5719, 0.5054, 0.6391, 0.1919, 0.5260]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_one_hot_encoded = torch.Tensor([[0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1]])\n",
    "\n",
    "batch_one_hot_encoded @ embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to learn\n",
      "[318, 3832, 298, 3178]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1219, -0.9217,  0.0804, -0.1049, -0.4088,  1.1438, -0.2155,  0.8280],\n",
       "        [-0.5726,  2.1356,  0.1306,  1.0591,  0.0731,  0.1786, -0.8104, -0.1310],\n",
       "        [ 0.0997,  0.6453,  2.4119,  0.5138,  0.2973,  0.0155, -0.8086,  0.3057],\n",
       "        [ 1.0756,  0.4220, -1.1676,  2.2978,  0.5456,  0.6231,  2.5453, -0.4455]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(56)\n",
    "### What is the attention mechanism?\n",
    "\n",
    "#### Attention allows the model to learn the affinities between the tokens\n",
    "#### Given the past (previous token), how can they interact to predict the most likely next token?\n",
    "\n",
    "# [I love to learn] ---> about\n",
    "print(\"I love to learn\")\n",
    "tokens = tokenizer.encode(\"I love to learn\", False, False)\n",
    "print(tokens)\n",
    "vocab_size = 32000\n",
    "embedding_size = 8\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "x = embedding_layer(torch.LongTensor(tokens))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I     ---> 318  ---> [-0.1219, -0.9217,  0.0804, -0.1049, -0.4088,  1.1438, -0.2155,  0.8280],\n",
    "# love  ---> 3832 ---> [-0.5726,  2.1356,  0.1306,  1.0591,  0.0731,  0.1786, -0.8104, -0.1310],\n",
    "# to    ---> 298  ---> [ 0.0997,  0.6453,  2.4119,  0.5138,  0.2973,  0.0155, -0.8086,  0.3057],\n",
    "# learn ---> 3178 ---> [ 1.0756,  0.4220, -1.1676,  2.2978,  0.5456,  0.6231,  2.5453, -0.4455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 4\n",
    "mask = torch.tril(torch.ones(context_length, context_length))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.zeros(context_length, context_length)\n",
    "weights = weights.masked_fill(mask == 0, float(\"-Inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1219, -0.9217,  0.0804, -0.1049, -0.4088,  1.1438, -0.2155,  0.8280],\n",
       "        [-0.3473,  0.6069,  0.1055,  0.4771, -0.1679,  0.6612, -0.5130,  0.3485],\n",
       "        [-0.1983,  0.6197,  0.8743,  0.4894, -0.0128,  0.4460, -0.6115,  0.3342],\n",
       "        [ 0.1202,  0.5703,  0.3638,  0.9415,  0.1268,  0.4903,  0.1777,  0.1393]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "### How can we get meaningfull weights for the attention?\n",
    "# queries and key will interact to decide the level of affinity between tokens\n",
    "torch.manual_seed(78)\n",
    "head_size = 4\n",
    "key = nn.Linear(embedding_size, head_size, bias=False)\n",
    "query = nn.Linear(embedding_size, head_size, bias=False)\n",
    "k = key(\n",
    "    x\n",
    ")  # (context_length, embedding_size) @ (embedding_size, head_size) => (context_length, head_size)\n",
    "q = query(x)\n",
    "print(k.shape, q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2632,  0.2153, -0.1248,  0.4342],\n",
      "        [-0.4048,  0.7153, -0.1434,  0.3645],\n",
      "        [-0.5860,  0.5796,  0.3083, -0.0288],\n",
      "        [ 1.0913, -0.8812,  0.7187, -0.1261]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.6620,  0.3543,  0.1199,  0.3901],\n",
      "        [-0.2861, -0.3934,  0.7692, -0.6666],\n",
      "        [ 0.1974,  0.6948,  0.3016,  0.3322],\n",
      "        [ 0.0445, -0.4471,  0.7331,  1.0798]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(k)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = q @ k.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4933, 0.5067, 0.0000, 0.0000],\n",
       "        [0.3059, 0.3686, 0.3255, 0.0000],\n",
       "        [0.2434, 0.1729, 0.1659, 0.4179]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights\n",
    "weights = weights.masked_fill(mask == 0, float(\"-Inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          I      love     to     learn\n",
    "# I     [1.0000, 0.0000, 0.0000, 0.0000]\n",
    "# love  [0.4933, 0.5067, 0.0000, 0.0000]\n",
    "# to    [0.3059, 0.3686, 0.3255, 0.0000]\n",
    "# learn [0.2434, 0.1729, 0.1659, 0.4179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2632,  0.2153, -0.1248,  0.4342],\n",
       "        [-0.0752,  0.4686, -0.1342,  0.3989],\n",
       "        [-0.2595,  0.5182,  0.0093,  0.2578],\n",
       "        [ 0.3529, -0.0960,  0.2963,  0.1112]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregating information from the values\n",
    "torch.manual_seed(78)\n",
    "values = nn.Linear(embedding_size, head_size, bias=False)\n",
    "v = values(x)\n",
    "output = weights @ v\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEAD 1                                     HEAD 2\n",
    "# [ 0.2632,  0.2153, -0.1248,  0.4342]       [... ... ... ...]\n",
    "# [-0.0752,  0.4686, -0.1342,  0.3989]   +   [... ... ... ...]\n",
    "# [-0.2595,  0.5182,  0.0093,  0.2578]       [... ... ... ...]\n",
    "# [ 0.3529, -0.0960,  0.2963,  0.1112]       [... ... ... ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Final layer to produce logits and probabilities\n",
    "\n",
    "output_to_project = torch.cat([output, output], dim=-1)\n",
    "\n",
    "final_projection = nn.Linear(embedding_size, vocab_size)\n",
    "logits = final_projection(output_to_project)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.6348e-05, 5.0826e-05, 3.4992e-05,  ..., 4.3651e-05, 3.5069e-05,\n",
       "        3.6860e-05], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "probs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we measure the performance of a model? The cross-entropy loss\n",
    "\n",
    "##### For each target token, we can retrieve the probability of the model to predict it\n",
    "##### In a batch, we have batch_size * context_length predictions tokens\n",
    "##### One way to measure the quality is to multiply all the model probability to predict the next token\n",
    "\n",
    "# token(s) [6974] need to predict 19055\n",
    "# token(s) [6974, 19055] need to predict 19006\n",
    "# token(s) [6974, 19055, 19006] need to predict 3820\n",
    "# token(s) [6974, 19055, 19006, 3820] need to predict 262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0643e-05, grad_fn=<SelectBackward0>)\n",
      "tensor(3.7046e-05, grad_fn=<SelectBackward0>)\n",
      "tensor(2.8668e-05, grad_fn=<SelectBackward0>)\n",
      "tensor(3.1741e-05, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs[0, 19055])\n",
    "print(probs[1, 19006])\n",
    "print(probs[2, 3820])\n",
    "print(probs[3, 262])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-41.1317, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To avoid numeric instability, we can use the log of the probabilities\n",
    "#  log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "(\n",
    "    torch.log(probs[0, 19055])\n",
    "    + torch.log(probs[1, 19006])\n",
    "    + torch.log(probs[2, 3820])\n",
    "    + torch.log(probs[3, 262])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41.1317, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because, we like to minimize the loss, we can use the negative log likelihood\n",
    "\n",
    "-(\n",
    "    torch.log(probs[0, 19055])\n",
    "    + torch.log(probs[1, 19006])\n",
    "    + torch.log(probs[2, 3820])\n",
    "    + torch.log(probs[3, 262])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we actually train the model?\n",
    "#### What is gradient descent and backpropagation?\n",
    "\n",
    "from wikiGPT.model import Transformer, ModelArgs\n",
    "\n",
    "model_config = ModelArgs(\n",
    "    dim=16,\n",
    "    n_layers=1,\n",
    "    n_heads=1,\n",
    "    vocab_size=32000,\n",
    "    hidden_dim=16,\n",
    "    max_context_length=10,\n",
    ")\n",
    "\n",
    "model = Transformer(model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 16)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (attn): Attention(\n",
      "        (wq): Linear(in_features=16, out_features=16, bias=False)\n",
      "        (wk): Linear(in_features=16, out_features=16, bias=False)\n",
      "        (wv): Linear(in_features=16, out_features=16, bias=False)\n",
      "        (wo): Linear(in_features=16, out_features=16, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=16, out_features=32, bias=False)\n",
      "        (w2): Linear(in_features=32, out_features=16, bias=False)\n",
      "        (w3): Linear(in_features=16, out_features=32, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_norm): RMSNorm()\n",
      "      (ff_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=16, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514608"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "params = [p for _, p in params.items()]\n",
    "sum(p.numel() for p in params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiGPT.iterate import TokenIterator\n",
    "from functools import partial\n",
    "\n",
    "# training loop\n",
    "iter_params = {\n",
    "        \"pretokenized_source\": Path(f\"wikiGPT/data/tok{model_config.vocab_size}\"),\n",
    "        \"context_length\": model_config.max_context_length,\n",
    "        # \"verbose\": True,\n",
    "    }\n",
    "iter_batches = partial(\n",
    "        TokenIterator.iter_batches,\n",
    "        batch_size=2,\n",
    "        device=\"cpu\",\n",
    "        num_workers=0,\n",
    "        **iter_params,\n",
    ")\n",
    "train_batch_iter = iter_batches(split=\"train\")\n",
    "X, Y = next(train_batch_iter)\n",
    "\n",
    "# define an optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    # if i > 0:\n",
    "    #     break\n",
    "    # project the input through the model\n",
    "    logits = model(X, Y)\n",
    "\n",
    "    # compute the loss\n",
    "    logits = logits.view(-1, logits.size(-1))\n",
    "    targets = Y.view(-1)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    # update the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# good resources on understanding backpropagation and gradient descent\n",
    "# https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=1\n",
    "# https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2\n",
    "# https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3\n",
    "# https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A note on model sizes and required training computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How do we make a prediction with the model?\n",
    "#### probabilistic sampling\n",
    "#### inefficiency of the attention mechanism at inference time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
