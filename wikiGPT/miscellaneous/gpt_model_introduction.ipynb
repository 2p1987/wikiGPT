{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a Generative Pretrained Transformer (GPT) model?\n",
    "\n",
    "The goal of this notebook is to explore the main aspects around training a generative AI model for text.\\\n",
    "We will review the main concepts & steps for training and talk also about how the prediction of new content happens.\n",
    "\n",
    "**Topics:**\n",
    "- Tokenization\n",
    "- Main model components\n",
    "- How the model performance is evaluated\n",
    "- How the model is trained\n",
    "- How the model is used to create new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = (\n",
    "    subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"])\n",
    "    .strip()\n",
    "    .decode(\"utf-8\")\n",
    ")\n",
    "os.chdir(repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The different phases of model training\n",
    "\n",
    "- Pre-training: the model understand the language logics but does not specifically know how to answer an instruction/question\n",
    "- Supervised fine-tuning: the model is trained to answer instruction\n",
    "- Alignement with human preferences (RLHF): the model is trained to align preferences with human preferences\n",
    "\n",
    "See slides: [pre-training](https://docs.google.com/presentation/d/1S8ao40-CdclRU0D2D9FdyN5x8fZL1Iv5/edit?_hsenc=p2ANqtz--zYcYKj9_o5fLbt_D3P4tzLanpAyfFm14Z2NXEvCZxbsjLtax9y5mYCzRg-opvXZhsYGEH#slide=id.g2b67c4de1ca_1_13), [supervised fine-tuning](https://docs.google.com/presentation/d/1S8ao40-CdclRU0D2D9FdyN5x8fZL1Iv5/edit?_hsenc=p2ANqtz--zYcYKj9_o5fLbt_D3P4tzLanpAyfFm14Z2NXEvCZxbsjLtax9y5mYCzRg-opvXZhsYGEH#slide=id.g2b67c4de1ca_1_59), [RLHF](https://docs.google.com/presentation/d/1S8ao40-CdclRU0D2D9FdyN5x8fZL1Iv5/edit?_hsenc=p2ANqtz--zYcYKj9_o5fLbt_D3P4tzLanpAyfFm14Z2NXEvCZxbsjLtax9y5mYCzRg-opvXZhsYGEH#slide=id.g2b67c4de1ca_1_77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wiki english\n",
    "import json\n",
    "\n",
    "with open(\"wikiGPT/data/shuffled_shards/shard_0.json\", \"r\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nowa Kiszewa  is a village in the administrative district of Gmina Kościerzyna, within Kościerzyna County, Pomeranian Voivodeship, in northern Poland. It lies approximately  south-east of Kościerzyna '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"content\"][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A naive tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 21, 34, 2, 33, 4, 34, 32, 18, 25, 29, 4, 18, 4, 10, 15, 2, 4, 28, 30, 20, 21, 27, 2]\n",
      "<unk>et<unk>s train a GP<unk> model<unk>\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(data[\"content\"])))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "\n",
    "def create_naive_tokenizer(text):\n",
    "    tokenizer = {vocab[i]: i + 3 for i in range(len(vocab))}\n",
    "    tokenizer[\"<s>\"] = 0\n",
    "    tokenizer[\"</s>\"] = 1\n",
    "    tokenizer[\"<unk>\"] = 2\n",
    "    detokenizer = {v: k for k, v in tokenizer.items()}\n",
    "    return tokenizer, detokenizer\n",
    "\n",
    "\n",
    "naive_tokenizer, naive_detokenizer = create_naive_tokenizer(data[\"content\"])\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tokenizer.get(letter, 2) for letter in text]\n",
    "\n",
    "\n",
    "def detokenize(tokens, detokenizer):\n",
    "    return \"\".join([detokenizer.get(token, \"<unk>\") for token in tokens])\n",
    "\n",
    "\n",
    "tokens = tokenize(\"Let's train a GPT model!\", naive_tokenizer)\n",
    "print(tokens)\n",
    "print(detokenize(tokens, naive_detokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tokenizer based on the Byte Pair Encoding (BPE) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love to learn about AI!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Byte pair encoding => use the data to tell how to tokenize\n",
    "\n",
    "# From a corpus of text to train the tokenizer\n",
    "# Choose a vocabulary size: i.e. the maximum number of tokens our vocabulary can have\n",
    "\n",
    "# Start with a character-level tokenizer from the corpus\n",
    "\"I love to learn about AI\"  # => [' ', 'A', 'I', 'a', 'b', 'e', 'l', 'n', 'o', 'r', 't', 'u', 'v']\n",
    "\n",
    "# Count the frequency of vocabulary items of characters in the corpus\n",
    "\"I love to learn about AI!\"  # => {\"I \": 1, \" l\": 2, \"lo\": 1, \"ov\": 1, ...}\n",
    "\n",
    "# Merge the most frequent pair of characters into a single token\n",
    "\"I love to learn about AI!\"  # => [' ', 'A', 'I', 'a', 'b', 'e', 'l', 'n', 'o', 'r', 't', 'u', 'v', ' l']\n",
    "\n",
    "# Repeat until the vocabulary size is reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[318, 3832, 298, 3178, 900, 14788, 19086]\n",
      "I love to learn about AI!\n"
     ]
    }
   ],
   "source": [
    "from wikiGPT.tokenize import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(Path(\"wikiGPT/tokenizers/tok32000.model\"))\n",
    "\n",
    "tokens = tokenizer.encode(\"I love to learn about AI!\", bos=False, eos=False)\n",
    "print(tokens)\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love to learn about AI!',\n",
       " \"Let's train a GPT model!\",\n",
       " 'Nowa Kiszewa  is a village in ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### A batch of data from human to machine reading\n",
    "\n",
    "# A batch is composed of two dimensions: the number of samples we are going to pass to the model and the context length\n",
    "\n",
    "corpus = [\n",
    "    \"I love to learn about AI!\",\n",
    "    \"Let's train a GPT model!\",\n",
    "    data[\"content\"][:30],\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[318, 3832, 298, 3178, 900, 14788, 19086],\n",
       " [6974, 19055, 19006, 3820, 262, 11867, 19028, 3005, 19086],\n",
       " [344, 4397, 385, 273, 15544, 19000, 351, 262, 1429, 280]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "context_length = 4\n",
    "\n",
    "# let's tokenize our text\n",
    "tokenized_corpus = [tokenizer.encode(text, False, False) for text in corpus]\n",
    "\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected sample: 0\n"
     ]
    }
   ],
   "source": [
    "# Creating a first batch for the next token prediction\n",
    "import random\n",
    "\n",
    "sample = random.choice(range(len(tokenized_corpus)))\n",
    "print(f\"Selected sample: {sample}\")\n",
    "\n",
    "batch = tokenized_corpus[sample][: context_length + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[318, 3832, 298, 3178, 900]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token(s) [318] need to predict 3832\n",
      "token(s) [318, 3832] need to predict 298\n",
      "token(s) [318, 3832, 298] need to predict 3178\n",
      "token(s) [318, 3832, 298, 3178] need to predict 900\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_length):\n",
    "    print(f\"token(s) {batch[:i+1]} need to predict {batch[i+1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subword(s) 'I' need to predict 'love'\n",
      "subword(s) 'I love' need to predict 'to'\n",
      "subword(s) 'I love to' need to predict 'learn'\n",
      "subword(s) 'I love to learn' need to predict 'about'\n"
     ]
    }
   ],
   "source": [
    "for i in range(context_length):\n",
    "    print(\n",
    "        f\"\"\"subword(s) '{tokenizer.decode(batch[:i+1])}' need to predict '{tokenizer.decode(batch[i+1])}'\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture: let's have a look at the main components of a text generation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, the initial embedding of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5374, 0.7877, 0.4807, 0.3653, 0.6011, 0.3297],\n",
      "        [0.4546, 0.0125, 0.9942, 0.6804, 0.5595, 0.0616],\n",
      "        [0.3252, 0.3354, 0.2572, 0.9170, 0.7575, 0.3402]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "### The embedding layer\n",
    "\n",
    "#### This is the initial layer that will transform the token (int) into a vector\n",
    "vocab = [0, 1, 2]\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 6\n",
    "\n",
    "embedding_layer = torch.rand(vocab_size, embedding_size)\n",
    "print(embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 6])\n",
      "tensor([[[0.4546, 0.0125, 0.9942, 0.6804, 0.5595, 0.0616],\n",
      "         [0.5374, 0.7877, 0.4807, 0.3653, 0.6011, 0.3297],\n",
      "         [0.5374, 0.7877, 0.4807, 0.3653, 0.6011, 0.3297],\n",
      "         [0.3252, 0.3354, 0.2572, 0.9170, 0.7575, 0.3402]]])\n"
     ]
    }
   ],
   "source": [
    "batch = [1, 0, 0, 2]\n",
    "batch_size = 1\n",
    "context_length = 4\n",
    "embedding = torch.zeros(batch_size, context_length, embedding_size)\n",
    "for i, token in enumerate(batch):\n",
    "    embedding[0, i] = embedding_layer[token]\n",
    "\n",
    "print(embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4546, 0.0125, 0.9942, 0.6804, 0.5595, 0.0616],\n",
       "        [0.5374, 0.7877, 0.4807, 0.3653, 0.6011, 0.3297],\n",
       "        [0.5374, 0.7877, 0.4807, 0.3653, 0.6011, 0.3297],\n",
       "        [0.3252, 0.3354, 0.2572, 0.9170, 0.7575, 0.3402]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_one_hot_encoded = torch.Tensor([[0, 1, 0], [1, 0, 0], [1, 0, 0], [0, 0, 1]])\n",
    "\n",
    "batch_one_hot_encoded @ embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The self-attention mechanism, at the heart of the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to learn\n",
      "[318, 3832, 298, 3178]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1219, -0.9217,  0.0804, -0.1049, -0.4088,  1.1438, -0.2155,  0.8280],\n",
       "        [-0.5726,  2.1356,  0.1306,  1.0591,  0.0731,  0.1786, -0.8104, -0.1310],\n",
       "        [ 0.0997,  0.6453,  2.4119,  0.5138,  0.2973,  0.0155, -0.8086,  0.3057],\n",
       "        [ 1.0756,  0.4220, -1.1676,  2.2978,  0.5456,  0.6231,  2.5453, -0.4455]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(56)\n",
    "### What is the attention mechanism?\n",
    "\n",
    "#### Attention allows the model to learn the affinities between the tokens\n",
    "#### Given the past (previous token), how can they interact to predict the most likely next token?\n",
    "\n",
    "# [I love to learn] ---> about\n",
    "print(\"I love to learn\")\n",
    "tokens = tokenizer.encode(\"I love to learn\", False, False)\n",
    "print(tokens)\n",
    "vocab_size = 32000\n",
    "embedding_size = 8\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_size)\n",
    "x = embedding_layer(torch.LongTensor(tokens))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I     ---> 318  ---> [-0.1219, -0.9217,  0.0804, -0.1049, -0.4088,  1.1438, -0.2155,  0.8280]\n",
    "# love  ---> 3832 ---> [-0.5726,  2.1356,  0.1306,  1.0591,  0.0731,  0.1786, -0.8104, -0.1310]\n",
    "# to    ---> 298  ---> [ 0.0997,  0.6453,  2.4119,  0.5138,  0.2973,  0.0155, -0.8086,  0.3057]\n",
    "# learn ---> 3178 ---> [ 1.0756,  0.4220, -1.1676,  2.2978,  0.5456,  0.6231,  2.5453, -0.4455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 4\n",
    "mask = torch.tril(torch.ones(context_length, context_length))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.zeros(context_length, context_length)\n",
    "weights = weights.masked_fill(mask == 0, float(\"-Inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1219, -0.9217,  0.0804, -0.1049, -0.4088,  1.1438, -0.2155,  0.8280],\n",
       "        [-0.3473,  0.6069,  0.1055,  0.4771, -0.1679,  0.6612, -0.5130,  0.3485],\n",
       "        [-0.1983,  0.6197,  0.8743,  0.4894, -0.0128,  0.4460, -0.6115,  0.3342],\n",
       "        [ 0.1202,  0.5703,  0.3638,  0.9415,  0.1268,  0.4903,  0.1777,  0.1393]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8]) torch.Size([4, 8])\n"
     ]
    }
   ],
   "source": [
    "### How can we get meaningfull weights for the attention?\n",
    "# queries and key will interact to decide the level of affinity between tokens\n",
    "torch.manual_seed(78)\n",
    "key = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "query = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "k = key(\n",
    "    x\n",
    ")  # (context_length, embedding_size) @ (embedding_size, embedding_size) => (context_length, embedding_size)\n",
    "q = query(x)\n",
    "print(k.shape, q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2632,  0.2153, -0.1248,  0.4342,  0.6620,  0.3543,  0.1199,  0.3901],\n",
      "        [-0.4048,  0.7153, -0.1434,  0.3645, -0.2861, -0.3934,  0.7692, -0.6666],\n",
      "        [-0.5860,  0.5796,  0.3083, -0.0288,  0.1974,  0.6948,  0.3016,  0.3322],\n",
      "        [ 1.0913, -0.8812,  0.7187, -0.1261,  0.0445, -0.4471,  0.7331,  1.0798]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "tensor([[-0.3480, -0.0343,  0.1525, -0.1166,  0.3183,  0.0807,  0.3076, -0.0275],\n",
      "        [ 0.3468,  0.8050, -0.4144,  0.2355,  0.0146, -0.3874, -0.5229,  0.4610],\n",
      "        [-0.0678,  1.0615,  0.4411,  1.0523,  0.1471,  0.2051,  0.2539,  0.6423],\n",
      "        [ 1.4488, -1.9597, -0.4880,  0.0270,  0.0287, -0.8733, -1.3105, -0.9306]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(k)\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = q @ k.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5960, 0.4040, 0.0000, 0.0000],\n",
       "        [0.3444, 0.2520, 0.4036, 0.0000],\n",
       "        [0.0996, 0.0306, 0.0069, 0.8629]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights\n",
    "weights = weights.masked_fill(mask == 0, float(\"-Inf\"))\n",
    "weights = F.softmax(weights, dim=-1)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          I      love     to     learn\n",
    "# I     [1.0000, 0.0000, 0.0000, 0.0000]\n",
    "# love  [0.5960, 0.4040, 0.0000, 0.0000]\n",
    "# to    [0.3444, 0.2520, 0.4036, 0.0000]\n",
    "# learn [0.0996, 0.0306, 0.0069, 0.8629]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2632,  0.2153, -0.1248,  0.4342,  0.6620,  0.3543,  0.1199,  0.3901],\n",
       "        [-0.0067,  0.4173, -0.1323,  0.4060,  0.2789,  0.0522,  0.3822, -0.0368],\n",
       "        [-0.2478,  0.4883,  0.0453,  0.2298,  0.2355,  0.3033,  0.3568,  0.1004],\n",
       "        [ 0.9514, -0.7130,  0.6055, -0.0546,  0.0970, -0.3577,  0.6701,  0.9525]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregating information from the values\n",
    "torch.manual_seed(78)\n",
    "values = nn.Linear(embedding_size, embedding_size, bias=False)\n",
    "v = values(x)\n",
    "output = weights @ v\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32000])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Final layer to produce logits and probabilities\n",
    "\n",
    "final_projection = nn.Linear(embedding_size, vocab_size)\n",
    "logits = final_projection(output)\n",
    "\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.8534e-05, 1.7943e-05, 2.7108e-05,  ..., 2.0701e-05, 3.7982e-05,\n",
       "        5.1995e-05], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = F.softmax(logits, dim=-1)\n",
    "probs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we measure the performance of a model? The cross-entropy loss\n",
    "\n",
    "##### For each target token, we can retrieve the probability of the model to predict it\n",
    "##### In a batch, we have batch_size * context_length predictions tokens\n",
    "##### One way to measure the quality is to multiply all the model probability to predict the next token\n",
    "\n",
    "# token(s) [6974] need to predict 19055\n",
    "# token(s) [6974, 19055] need to predict 19006\n",
    "# token(s) [6974, 19055, 19006] need to predict 3820\n",
    "# token(s) [6974, 19055, 19006, 3820] need to predict 262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0201e-05, grad_fn=<SelectBackward0>)\n",
      "tensor(2.8422e-05, grad_fn=<SelectBackward0>)\n",
      "tensor(3.3656e-05, grad_fn=<SelectBackward0>)\n",
      "tensor(1.0244e-05, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs[0, 19055])\n",
    "print(probs[1, 19006])\n",
    "print(probs[2, 3820])\n",
    "print(probs[3, 262])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-41.1317, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To avoid numeric instability, we can use the log of the probabilities\n",
    "#  log(a*b*c) = log(a) + log(b) + log(c)\n",
    "\n",
    "(\n",
    "    torch.log(probs[0, 19055])\n",
    "    + torch.log(probs[1, 19006])\n",
    "    + torch.log(probs[2, 3820])\n",
    "    + torch.log(probs[3, 262])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(41.1317, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because, we like to minimize the loss, we can use the negative log likelihood\n",
    "\n",
    "-(\n",
    "    torch.log(probs[0, 19055])\n",
    "    + torch.log(probs[1, 19006])\n",
    "    + torch.log(probs[2, 3820])\n",
    "    + torch.log(probs[3, 262])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "### How can we actually train the model?\n",
    "#### What is gradient descent and backpropagation?\n",
    "\n",
    "from wikiGPT.model import Transformer, ModelArgs\n",
    "\n",
    "model_config = ModelArgs(\n",
    "    dim=64,\n",
    "    n_layers=1,\n",
    "    n_heads=2,\n",
    "    vocab_size=32000,\n",
    "    hidden_dim=64,\n",
    "    max_context_length=10,\n",
    ")\n",
    "\n",
    "model = Transformer(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (tok_embeddings): Embedding(32000, 64)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerBlock(\n",
      "      (attn): Attention(\n",
      "        (wq): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wk): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wv): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (wo): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (feed_forward): FeedForward(\n",
      "        (w1): Linear(in_features=64, out_features=96, bias=False)\n",
      "        (w2): Linear(in_features=96, out_features=64, bias=False)\n",
      "        (w3): Linear(in_features=64, out_features=96, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (attn_norm): RMSNorm()\n",
      "      (ff_norm): RMSNorm()\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      "  (output): Linear(in_features=64, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model params: 2,083,008\n"
     ]
    }
   ],
   "source": [
    "params = {pn: p for pn, p in model.named_parameters() if p.requires_grad}\n",
    "params = [p for _, p in params.items()]\n",
    "print(f\"Number of model params: {sum(p.numel() for p in params):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikiGPT.iterate import TokenIterator\n",
    "from functools import partial\n",
    "\n",
    "# training loop\n",
    "iter_params = {\n",
    "    \"pretokenized_source\": Path(f\"wikiGPT/data/tok{model_config.vocab_size}\"),\n",
    "    \"context_length\": model_config.max_context_length,\n",
    "    # \"verbose\": True,\n",
    "}\n",
    "iter_batches = partial(\n",
    "    TokenIterator.iter_batches,\n",
    "    batch_size=2,\n",
    "    device=\"cpu\",\n",
    "    num_workers=0,\n",
    "    **iter_params,\n",
    ")\n",
    "train_batch_iter = iter_batches(split=\"train\")\n",
    "X, Y = next(train_batch_iter)\n",
    "\n",
    "# define an optimizer\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good introduction videos gradient descent and on backpropagation from 3blue1brown youtube channel\n",
    "[What is a neural network](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=1)\\\n",
    "[What is gradient descent](https://www.youtube.com/watch?v=IHZwWFHWa-w&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=2)\\\n",
    "[Backpropagation intuitive explanation](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3)\\\n",
    "[Backpropagation computation](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # if i > 0:\n",
    "    #     break\n",
    "\n",
    "    # project the input through the model\n",
    "    logits = model(X, Y)\n",
    "\n",
    "    # compute the loss\n",
    "    logits = logits.view(-1, logits.size(-1))\n",
    "    targets = Y.view(-1)\n",
    "    loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    # update the weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # get next batch of data\n",
    "    X, Y = next(train_batch_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.7052, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we generate new text with the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1, 952]])\n",
      "tensor([[  1, 952, 370]])\n",
      "tensor([[  1, 952, 370, 262]])\n",
      "tensor([[   1,  952,  370,  262, 9294]])\n",
      "tensor([[   1,  952,  370,  262, 9294,  396]])\n",
      "tensor([[   1,  952,  370,  262, 9294,  396, 3408]])\n",
      "tensor([[    1,   952,   370,   262,  9294,   396,  3408, 31157]])\n",
      "tensor([[    1,   952,   370,   262,  9294,   396,  3408, 31157, 19034]])\n",
      "tensor([[    1,   952,   370,   262,  9294,   396,  3408, 31157, 19034,   317]])\n",
      "tensor([[    1,   952,   370,   262,  9294,   396,  3408, 31157, 19034,   317,\n",
      "           667]])\n",
      "['oss with a CBS he below딪I l all']\n"
     ]
    }
   ],
   "source": [
    "temperature = 1.0\n",
    "top_k = None\n",
    "\n",
    "start_ids = tokenizer.encode(\"\", bos=True, eos=False)\n",
    "x = torch.tensor(start_ids, dtype=torch.long, device=\"cpu\")[None, ...]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for _ in range(10):\n",
    "        logits = model(x)\n",
    "        logits = logits[:, -1, :]  # crop to just the final time step\n",
    "\n",
    "        if temperature > 0:\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float(\"Inf\")\n",
    "\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        else:\n",
    "            _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        x = torch.cat((x, idx_next), dim=1)\n",
    "        print(x)\n",
    "\n",
    "    print(tokenizer.decode(x.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
